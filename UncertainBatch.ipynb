{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9cfa1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:33:14.990533Z",
     "start_time": "2024-06-25T07:33:13.655504Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from skmultilearn.dataset import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import random\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from skmultilearn.dataset import load_from_arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sklearn.metrics as metrics\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import itertools\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pdb\n",
    "import scipy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "import math\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from util import *\n",
    "from layers import  *\n",
    "from model import *\n",
    "from function import *\n",
    "from scipy.stats import pearsonr\n",
    "from collections import deque\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.integrate import dblquad\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c237c19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:33:15.002335Z",
     "start_time": "2024-06-25T07:33:14.993160Z"
    }
   },
   "outputs": [],
   "source": [
    "#####seed####\n",
    "def seed_all(seed): \n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "seed=42\n",
    "seed_all(seed)\n",
    "device = torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8699f700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:33:15.012834Z",
     "start_time": "2024-06-25T07:33:15.003483Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    def __init__(self,name,X,y):\n",
    "        self.name=name      \n",
    "        self.X_train=X\n",
    "        self.y_train=y\n",
    "        self.configs={} \n",
    "    def getconfig(self):\n",
    "        self.configs['label_matrix']=np.array(self.y_train)\n",
    "        self.configs['num_classes']=self.y_train.shape[1] \n",
    "        self.configs['num_ins']=self.X_train.shape[0] \n",
    "        self.configs['seed']=42 \n",
    "        self.configs['batch_size']=128\n",
    "        self.configs['epoch']=100 \n",
    "        self.configs['lr']=1e-4\n",
    "        self.configs['device']=torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.configs['weight']=limb(np.array(self.X_train),np.array(self.y_train))\n",
    "        self.configs['extra_sample']=int(self.X_train.shape[0]*0.1)\n",
    "        self.configs['min_ins_idx']=minority_instance(np.array(self.y_train))\n",
    "        self.configs['minority_label_indices'],_=Labeltype(np.array(self.y_train))\n",
    "        self.configs['weight_list']=calweight(np.array(self.X_train),np.array(self.y_train))\n",
    "        self.configs['card'],_=CardAndDens(np.array(self.X_train),np.array(self.y_train))\n",
    "        #DELA\n",
    "        if self.name=='DELA':\n",
    "            self.configs['in_features']=self.X_train.shape[1] \n",
    "            self.configs['latent_dim']=math.ceil(self.X_train.shape[1]/2)    \n",
    "            self.configs['lr_ratio']=0.8\n",
    "            self.configs['drop_ratio']=0.2\n",
    "            self.configs['tau']=2/3\n",
    "            self.configs['beta']=1e-4\n",
    "            self.configs['out_index']=-1\n",
    "        if self.name=='CLIF':\n",
    "            self.configs['class_emb_size']=self.y_train.shape[1]  \n",
    "            self.configs['input_x_size']=self.X_train.shape[1] \n",
    "            self.configs['num_layers']=2 \n",
    "            self.configs['in_layers']=3 \n",
    "            self.configs['hidden_list']=[math.ceil(self.y_train.shape[1]/2)]  \n",
    "            self.configs['out_index']=0        \n",
    "        if self.name=='PACA':\n",
    "            self.configs['drop_ratio']=0.1\n",
    "            self.configs['latent_dim']=math.ceil(self.X_train.shape[1]/2)\n",
    "            self.configs['in_features']=self.X_train.shape[1] #输入x的维度\n",
    "            self.configs['rand_seed']=self.configs['seed']\n",
    "            self.configs['eps']=1e-8    \n",
    "            self.configs['lr_scheduler']='fix'\n",
    "            self.configs['binary_data']=False\n",
    "            self.configs['weight_decay']=1e-5\n",
    "            self.configs['alpha']=2\n",
    "            self.configs['gamma']=10\n",
    "            self.configs['scheduler_warmup_epoch']=5\n",
    "            self.configs['scheduler_decay_epoch']=10\n",
    "            self.configs['scheduler_decay_rate']=1e-5 \n",
    "            self.configs['out_index']=-2\n",
    "        return self.configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e67a79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T07:33:15.619478Z",
     "start_time": "2024-06-25T07:33:15.595757Z"
    },
    "code_folding": [
     2,
     13,
     22,
     38,
     47,
     53,
     67,
     69,
     71
    ]
   },
   "outputs": [],
   "source": [
    "seed_all(seed)\n",
    "device = torch.device('cuda')\n",
    "def FeatureSelect(X,p):\n",
    "    if p==1:\n",
    "        return X.toarray(),feature_names\n",
    "    else:\n",
    "        featurecount=int(X.shape[1]*p)\n",
    "        Selectfeatureindex=[x[0] for x in (sorted(enumerate(X.sum(axis=0).tolist()[0]),key=lambda x: x[1],reverse=True))][:featurecount]\n",
    "        Allfeatureindex=[i for i in range(X.shape[1])]\n",
    "        featureindex=[i for i in Allfeatureindex if i not in Selectfeatureindex]\n",
    "        new_x=np.delete(X.toarray(),featureindex,axis=1)\n",
    "        new_featurename=[feature_names[i] for i in Selectfeatureindex] \n",
    "        return new_x,new_featurename\n",
    "def LabelSelect(y):\n",
    "    b=[]\n",
    "    new_labelname=[i for i in label_names]\n",
    "    for i in range(y.shape[1]):\n",
    "        if y[:,i].sum()<=20:\n",
    "            b.append(i)\n",
    "            new_labelname.remove(label_names[i])\n",
    "    new_y=np.delete(y.toarray(),b,axis=1)\n",
    "    return new_y,new_labelname\n",
    "def macro_averaging_auc(Y, P, O):\n",
    "    n = (Y.shape[0] + O.shape[0]) // 2\n",
    "    l = (Y.shape[1] + O.shape[1]) // 2\n",
    "\n",
    "    p = np.zeros(l)\n",
    "    q = np.sum(Y, 0)\n",
    "\n",
    "    zero_column_count = np.sum(q == 0)\n",
    "#     print(f\"all zero for label: {zero_column_count}\")\n",
    "    r, c = np.nonzero(Y)\n",
    "    for i, j in zip(r, c):\n",
    "        p[j] += np.sum((Y[ : , j] < 0.5) * (O[ : , j] <= O[i, j]))\n",
    "\n",
    "    i = (q > 0) * (q < n)\n",
    "\n",
    "    return np.sum(p[i] / (q[i] * (n - q[i]))) / l\n",
    "def hamming_loss(Y, P, O):\n",
    "    n = (Y.shape[0] + P.shape[0]) // 2\n",
    "    l = (Y.shape[1] + P.shape[1]) // 2\n",
    "\n",
    "    s1 = np.sum(Y, 1)\n",
    "    s2 = np.sum(P, 1)\n",
    "    ss = np.sum(Y * P, 1)\n",
    "\n",
    "    return np.sum(s1 + s2 - 2 * ss) / (n * l)\n",
    "def one_error(Y, P, O):\n",
    "    n = (Y.shape[0] + O.shape[0]) // 2\n",
    "\n",
    "    i = np.argmax(O, 1)\n",
    "\n",
    "    return np.sum(1 - Y[range(n), i]) / n\n",
    "def ranking_loss(Y, P, O):\n",
    "    n = (Y.shape[0] + O.shape[0]) // 2\n",
    "    l = (Y.shape[1] + O.shape[1]) // 2\n",
    "\n",
    "    p = np.zeros(n)\n",
    "    q = np.sum(Y, 1)\n",
    "\n",
    "    r, c = np.nonzero(Y)\n",
    "    for i, j in zip(r, c): \n",
    "        p[i] += np.sum((Y[i, : ] < 0.5) * (O[i, : ] >= O[i, j]))\n",
    "\n",
    "    i = (q > 0) * (q < l)\n",
    "\n",
    "    return np.sum(p[i] / (q[i] * (l - q[i]))) / n\n",
    "def micro_f1(Y, P, O):\n",
    "    return f1_score(Y, P, average='micro')\n",
    "def macro_f1(Y, P, O):\n",
    "    return f1_score(Y, P, average='macro')\n",
    "def eval_metrics(mod, metrics, datasets, idx,batch_size,device):\n",
    "    res_dict = {}\n",
    "    mod.eval()\n",
    "    y_true_list = []\n",
    "    y_scores_list = []\n",
    "    test_dataloader = DataLoader(datasets, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    for x, y in test_dataloader:\n",
    "        _, y_pred=mod.predict(x)\n",
    "#         pdb.set_trace()\n",
    "        y_true_list.append(y.cpu().numpy())\n",
    "        y_scores_list.append(y_pred.cpu().numpy())\n",
    "    y_true = np.vstack(y_true_list)\n",
    "    y_prob = np.vstack(y_scores_list)\n",
    "    y_pred = np.round(y_prob).astype(int)\n",
    "    res_dict1 = {metric.__name__: metric(y_true, y_pred,y_prob) for metric in metrics}\n",
    "#         # Calculate metric.\n",
    "#         res_dict1 = {metric.__name__: metric(y_true, y_pred) for metric in metrics[:2]}\n",
    "#         res_dict2 = {metric.__name__: metric(y_true, y_prob) for metric in metrics[2:5]}\n",
    "#         res_dict1.update(res_dict2)\n",
    "#     res_dict[f'dataset_{ix}']=res_dict1\n",
    "#         res_dict[f'dataset_{ix}'] = {metric.__name__: metric(y_true, y_pred) for metric in metrics[:8]}\n",
    "#         res_dict[f'dataset_{ix}'] = {metric.__name__: metric(y_true, y_prob) for metric in metrics[8:9]}\n",
    "    return res_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94dbdad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T08:05:53.812892Z",
     "start_time": "2024-06-25T08:05:53.793440Z"
    },
    "code_folding": [
     68,
     72,
     86
    ]
   },
   "outputs": [],
   "source": [
    "def update_H(H, y_pred, ids, max_history_length=5):\n",
    "    y_pred_numpy = y_pred.detach().cpu().numpy()\n",
    "    for i, idx in enumerate(ids):\n",
    "        if idx not in H:\n",
    "            H[idx] = deque(maxlen=max_history_length) \n",
    "        H[idx].append(y_pred_numpy[i])  \n",
    "    return H\n",
    "def update_E(H,E,ids,label_dim):\n",
    "    \n",
    "    for idx in ids:\n",
    "        current_predictions_history = np.array(H[idx])\n",
    "        last_row_index = len(current_predictions_history) - 1\n",
    "        for j in range(label_dim): \n",
    "            diffs = np.abs(np.diff(current_predictions_history[:, j]))\n",
    "            mean_diffs = np.sum(diffs)/len(diffs)\n",
    "            current_entropy = -1 / np.log(2) * (current_predictions_history[last_row_index][j]  * np.log(current_predictions_history[last_row_index][j]) + (1 - current_predictions_history[last_row_index][j] ) * np.log(1 -current_predictions_history[last_row_index][j] ))\n",
    "            E[idx,j]=1/2*mean_diffs+1/2* current_entropy\n",
    "    return E\n",
    "\n",
    "def update_U(E, U, epoch):\n",
    "    E[E > 1] = 1\n",
    "    if epoch>=5:\n",
    "        bins = np.array([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        discrete_values = np.array([0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "        indices = np.digitize(E, bins) - 1\n",
    "        indices = np.clip(indices, 0, len(discrete_values) - 1)\n",
    "        E = discrete_values[indices]\n",
    "#         I=np.abs(calculate_pearson_matrix(E))\n",
    "        \n",
    "    else:\n",
    "        I = np.ones((E.shape[1],E.shape[1]))\n",
    "#     if epoch==30:\n",
    "#         np.save('yahoo-Business1I(30).npy', I)\n",
    "    I = np.ones((E.shape[1],E.shape[1]))\n",
    "    U = np.dot(E, I)\n",
    "    w = np.sum(U, axis=1)\n",
    "    return w\n",
    "\n",
    "\n",
    "def calculate_probabilities(U, epoch, e_end, e0, se0):\n",
    "    # Delta is set to 1/N\n",
    "    N=len(U)\n",
    "    Delta = 1 / N \n",
    "    # Exponentially decay the selection pressure as training progresses\n",
    "    se = se0 * (np.exp(np.log(1 / se0) / (e_end - e0)))**(epoch - e0)\n",
    "    U_min = np.min(U)\n",
    "    U_max = np.max(U)\n",
    "    U_normalized = (U - U_min) / (U_max - U_min)\n",
    "    quantized_indices = np.ceil((1 - U_normalized) / Delta).astype(int)\n",
    "#     pdb.set_trace()\n",
    "    # Calculate the exponent for the sampling probability\n",
    "    exponent = -np.log(se) / N * quantized_indices \n",
    "    # Calculate the unnormalized probabilities\n",
    "    unnormalized_probabilities = np.exp(exponent)\n",
    "    return unnormalized_probabilities\n",
    "\n",
    "\n",
    "def calculate_mutual_information_matrix(data):\n",
    "    n, q = data.shape\n",
    "    mi_matrix = np.zeros((q, q))\n",
    "    for i in range(q):\n",
    "        for j in range(q):\n",
    "            if i == j:\n",
    "                mi_matrix[i, j] = 1  # 对角线置为1\n",
    "            else:\n",
    "                mi_matrix[i, j] = mutual_info_classif(data[:, i].reshape(-1, 1), data[:, j])[0]\n",
    "                mi_matrix[i, j] = max(mi, 0)\n",
    "    return mi_matrix\n",
    "\n",
    "def calculate_pearson_matrix(data):\n",
    "    pearson_matrix = np.corrcoef(data, rowvar=False)\n",
    "    return pearson_matrix\n",
    "\n",
    "def entropy_based_mutual_information_matrix(data):\n",
    "    n, q = data.shape\n",
    "    mi_matrix = np.zeros((q, q))\n",
    "    \n",
    "    for i in range(q):\n",
    "        for j in range(i, q):\n",
    "            if i == j:\n",
    "                mi_matrix[i, j] = 1\n",
    "            else:\n",
    "                mi = entropy_based_mutual_information(data[:, i], data[:, j])\n",
    "                mi_matrix[i, j] = mi\n",
    "                mi_matrix[j, i] = mi\n",
    "    \n",
    "    # Normalize each row by its maximum value\n",
    "    row_max = mi_matrix.max(axis=1, keepdims=True)\n",
    "    mi_matrix = mi_matrix / row_max\n",
    "    \n",
    "    return mi_matrix\n",
    "\n",
    "def entropy_based_mutual_information(x, y):\n",
    "    # Calculate mutual information between x and y using entropy-based methods\n",
    "    # Ensure x and y are flattened to 1D arrays\n",
    "    x = np.ravel(x)\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    # Calculate individual entropies\n",
    "    h_x = entropy(np.histogram(x, bins='auto')[0])\n",
    "    h_y = entropy(np.histogram(y, bins='auto')[0])\n",
    "    \n",
    "    # Calculate joint entropy\n",
    "    joint_hist, _, _ = np.histogram2d(x, y, bins='auto')\n",
    "    joint_entropy = entropy(joint_hist.ravel())\n",
    "    \n",
    "    # Mutual information\n",
    "    mi = h_x + h_y - joint_entropy\n",
    "    return mi\n",
    "\n",
    "class CustomDataLoader:\n",
    "    def __init__(self, dataset, net, configs,H,E,U,sample_probabilities,warm_epoch):\n",
    "        self.shuffle = False\n",
    "        self.dataset = dataset\n",
    "        self.net = net\n",
    "        self.batch_size = configs['batch_size']\n",
    "        self.sample_probabilities =sample_probabilities\n",
    "        self.dataset_indices=list(range(len(self.dataset)))\n",
    "        self.warm_epoch=warm_epoch   \n",
    "        \n",
    "        self.H=H\n",
    "        self.E=E\n",
    "        self.U=U    \n",
    "                              \n",
    "    def __iter__(self):\n",
    "        if self.current_epoch < self.warm_epoch:\n",
    "            batch_counter = 0\n",
    "            for start_idx in range(0, len(self.dataset_indices), self.batch_size):\n",
    "                end_idx = min(start_idx + self.batch_size, len(self.dataset_indices))\n",
    "                batch_indices = self.dataset_indices[start_idx:end_idx]\n",
    "                batch_counter += 1\n",
    "                yield self.get_data_from_indices(batch_indices)\n",
    "        else:\n",
    "            all_indices = self.dataset_indices  \n",
    "            all_probabilities = self.sample_probabilities   \n",
    "\n",
    "            total_sum = all_probabilities.sum()\n",
    "            all_probabilities /= total_sum\n",
    "\n",
    "            total_samples = len(self.dataset)\n",
    "            num_batches = np.ceil(total_samples / self.batch_size).astype(int)\n",
    "\n",
    "            for _ in range(num_batches):\n",
    "                num_required = self.batch_size \n",
    "                if num_required > 0:\n",
    "                    chosen_indices = np.random.choice(all_indices, size=num_required, replace=False, p=all_probabilities)\n",
    "                else:\n",
    "                    chosen_indices = []\n",
    "                batch_indices = list(chosen_indices)\n",
    "                yield self.get_data_from_indices(batch_indices)\n",
    "                \n",
    "    def get_data_from_indices(self, indices):\n",
    "        x, y = zip(*[self.dataset[i] for i in indices])\n",
    "        return indices, torch.stack(x), torch.stack(y)\n",
    "    def set_epoch(self, epoch):\n",
    "        self.current_epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863e5b3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-25T07:32:24.372Z"
    }
   },
   "outputs": [],
   "source": [
    "def training(configs,warm_epoch):\n",
    "    net=CLIFModel(configs).to(device)\n",
    "    num_epochs =configs['epoch']\n",
    "    batch_size = configs['batch_size']\n",
    "    card=int(configs['card'])\n",
    "#     card=configs['num_classes']\n",
    "    lr = 1e-4\n",
    "    \n",
    "    ins_dim=configs['num_ins']\n",
    "    label_dim=configs['num_classes']\n",
    "    \n",
    "    weight_decay = 1e-4  \n",
    "    betas = (0.9, 0.999) \n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr,  \n",
    "                           betas=betas, weight_decay=weight_decay)\n",
    "    writer = SummaryWriter(comment=f'{dataname}')\n",
    "\n",
    "    sample_probabilities=[]\n",
    "    \n",
    "    sample_indices=list(range(ins_dim))\n",
    "    H = {idx: deque(maxlen=5) for idx in sample_indices}\n",
    "    E= np.zeros((ins_dim, label_dim))\n",
    "    U = np.zeros(ins_dim)\n",
    "    \n",
    "    \n",
    "    custom_dataloader = CustomDataLoader(train_dataset, net=net, configs=configs,H=H,E=E,U=U,sample_probabilities=sample_probabilities,warm_epoch=warm_epoch)\n",
    "    validation_dataset, test_dataset_new = train_test_split(test_dataset, test_size=0.5, random_state=42)   \n",
    "    test_dataloader = DataLoader(test_dataset_new, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    best_auc = 0\n",
    "    best_model_state = None\n",
    "    epoch_losses_train = []\n",
    "\n",
    "    warmup_epochs =warm_epoch\n",
    "    total_steps = num_epochs * int(configs['num_ins']/batch_size)\n",
    "    warmup_steps = warmup_epochs * int(configs['num_ins']/batch_size)\n",
    "    global_step = 0\n",
    "    def update_learning_rate(optimizer, global_step, warmup_steps=warmup_steps, base_lr=lr):\n",
    "        if global_step < warmup_steps:\n",
    "            lr = base_lr * (global_step / warmup_steps)\n",
    "        else:\n",
    "            lr = base_lr\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr \n",
    "            \n",
    "    pcorrelation_matrix=np.zeros((label_dim, label_dim))\n",
    "    for epoch in range(num_epochs+1): \n",
    "        net.train()\n",
    "        batch_counter = 0   \n",
    "        loss_tracker = 0.0\n",
    "        loss_tracker2=0.0\n",
    "        all_individual_losses = {}\n",
    "        custom_dataloader.set_epoch(epoch) \n",
    "        for idx, x, y in custom_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(x)\n",
    "            loss_dict = net.loss_function_train(outputs, y)\n",
    "            loss = loss_dict['Loss']\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            update_learning_rate(optimizer, global_step) \n",
    "            custom_dataloader.H=update_H(custom_dataloader.H, torch.sigmoid(outputs[0]), idx)\n",
    "            custom_dataloader.E=update_E(custom_dataloader.H, custom_dataloader.E,idx,label_dim)\n",
    "            batch_counter+=1 \n",
    "        custom_dataloader.U=update_U(custom_dataloader.E, custom_dataloader.U, epoch)\n",
    "        custom_dataloader.sample_probabilities=calculate_probabilities(custom_dataloader.U, epoch, 10, 100, 8)    \n",
    "#         custom_dataloader.E=update_E(custom_dataloader.H, custom_dataloader.E,idx,label_dim)\n",
    "#         pcorrelation_matrix=calculate_pearson_correlation(custom_dataloader.E)\n",
    "        writer.add_scalar('train/loss', loss_tracker, epoch)\n",
    "        epoch_losses_train.append(loss_tracker /batch_counter)\n",
    "\n",
    "        net.eval()\n",
    "        y_true_list = []\n",
    "        y_scores_list = []\n",
    "        loss_tracker = 0.0\n",
    "        batch_counter = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in validation_dataloader:\n",
    "                _, y_pred=net.predict(x)\n",
    "                y_true_list.append(y.cpu().numpy())\n",
    "                y_scores_list.append(y_pred.cpu().numpy())\n",
    "                \n",
    "                outputs = net(x)\n",
    "                loss_dict = net.loss_function_train(outputs, y)\n",
    "                loss = loss_dict['Loss']\n",
    "                loss_tracker += loss.item()\n",
    "                batch_counter += 1\n",
    "        y_true = np.vstack(y_true_list)\n",
    "        y_scores = np.vstack(y_scores_list)\n",
    "\n",
    "        auc = macro_averaging_auc(y_true,y_scores, y_scores)\n",
    "#         writer.add_scalar('val/auc', auc, epoch)\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_epoch=epoch\n",
    "            best_model_state = net.state_dict().copy()\n",
    "    net.load_state_dict(best_model_state)\n",
    "    mets = eval_metrics(net, [macro_f1, micro_f1, macro_averaging_auc, ranking_loss, hamming_loss, one_error], test_dataset_new, configs['out_index'],configs['batch_size'],torch.device('cuda'))\n",
    "    return mets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f812f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-25T07:31:59.006Z"
    }
   },
   "outputs": [],
   "source": [
    "path_to_arff_files = [\"emotions\",\"scene\",\"yeast\", \"Corel5k\",\"rcv1subset1\",\"rcv1subset2\",\"rcv1subset3\",\"yahoo-Business1\",\"yahoo-Arts1\",\"bibtex\",'tmc2007','enron','cal500','LLOG-F']\n",
    "label_counts = [6, 6,14,374,101,101,101,28,25,159,22,53,174,75]\n",
    "select_feature=[1,1,1,1,0.02,0.02,0.02,0.05,0.05,1,0.01,1,1,1]\n",
    "\n",
    "for idx, dataname in enumerate(path_to_arff_files):\n",
    "    path_to_arff_file = f\"/home/tt/{dataname}.arff\"\n",
    "    X, y, feature_names, label_names = load_from_arff(\n",
    "        path_to_arff_file,\n",
    "        label_count=label_counts[idx],\n",
    "        label_location=\"end\",\n",
    "        load_sparse=False,\n",
    "        return_attribute_definitions=True\n",
    "        )\n",
    "    X,feature_names=FeatureSelect(X,select_feature[idx])  \n",
    "    y,label_names=LabelSelect(y)\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    warm_epoch=10\n",
    "    print(dataname)\n",
    "    k_fold = IterativeStratification(n_splits=5,order=1,random_state=42)\n",
    "    dicts=[]\n",
    "    for idx,(train,test) in enumerate(k_fold.split(X,y)):\n",
    "        train_dataset = TensorDataset(torch.tensor(X[train], device=device, dtype=torch.float),torch.tensor(y[train], device=device,dtype=torch.float))\n",
    "        test_dataset = TensorDataset(torch.tensor(X[test], device=device, dtype=torch.float), torch.tensor(y[test], device=device, dtype=torch.float))    \n",
    "        configs=CFG('CLIF',X[train],y[train]).getconfig()\n",
    "        dict_1=training(configs,warm_epoch)\n",
    "        dicts.append(dict_1)\n",
    "    averages_and_stds = {}\n",
    "    for key in dicts[0].keys():\n",
    "        values = [d[key] for d in dicts]\n",
    "        averages_and_stds[key] = {\n",
    "            'average': round(np.mean(values),4),\n",
    "            'std': round(np.std(values),4)\n",
    "        }\n",
    "    print(averages_and_stds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
