{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d9cfa1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T12:59:56.568296Z",
     "start_time": "2024-07-31T12:59:55.282883Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from skmultilearn.dataset import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import random\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from skmultilearn.dataset import load_from_arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sklearn.metrics as metrics\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import itertools\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pdb\n",
    "import scipy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "import math\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from util import *\n",
    "from layers import  *\n",
    "from model import *\n",
    "from function import *\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import pearsonr\n",
    "from collections import deque\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.integrate import dblquad\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c237c19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T12:59:56.579981Z",
     "start_time": "2024-07-31T12:59:56.570768Z"
    }
   },
   "outputs": [],
   "source": [
    "#####seed####\n",
    "def seed_all(seed): \n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "seed=42\n",
    "seed_all(seed)\n",
    "device = torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8699f700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T12:59:56.589902Z",
     "start_time": "2024-07-31T12:59:56.581099Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    def __init__(self,name,X,y):\n",
    "        self.name=name      \n",
    "        self.X_train=X\n",
    "        self.y_train=y\n",
    "        self.configs={} \n",
    "    def getconfig(self):\n",
    "        self.configs['label_matrix']=np.array(self.y_train)\n",
    "        self.configs['num_classes']=self.y_train.shape[1] \n",
    "        self.configs['num_ins']=self.X_train.shape[0] \n",
    "        self.configs['seed']=42 \n",
    "        self.configs['batch_size']=128\n",
    "        self.configs['epoch']=100 \n",
    "        self.configs['lr']=1e-4\n",
    "        self.configs['device']=torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.configs['weight']=limb(np.array(self.X_train),np.array(self.y_train))\n",
    "        self.configs['extra_sample']=int(self.X_train.shape[0]*0.1)\n",
    "        self.configs['min_ins_idx']=minority_instance(np.array(self.y_train))\n",
    "        self.configs['minority_label_indices'],_=Labeltype(np.array(self.y_train))\n",
    "        self.configs['weight_list']=calweight(np.array(self.X_train),np.array(self.y_train))\n",
    "        self.configs['card'],_=CardAndDens(np.array(self.X_train),np.array(self.y_train))\n",
    "        #DELA\n",
    "        if self.name=='DELA':\n",
    "            self.configs['in_features']=self.X_train.shape[1] \n",
    "            self.configs['latent_dim']=math.ceil(self.X_train.shape[1]/2)    \n",
    "            self.configs['lr_ratio']=0.8\n",
    "            self.configs['drop_ratio']=0.2\n",
    "            self.configs['tau']=2/3\n",
    "            self.configs['beta']=1e-4\n",
    "            self.configs['out_index']=-1\n",
    "        if self.name=='CLIF':\n",
    "            self.configs['class_emb_size']=self.y_train.shape[1]  \n",
    "            self.configs['input_x_size']=self.X_train.shape[1] \n",
    "            self.configs['num_layers']=2 \n",
    "            self.configs['in_layers']=3 \n",
    "            self.configs['hidden_list']=[math.ceil(self.y_train.shape[1]/2)]  \n",
    "            self.configs['out_index']=0        \n",
    "        if self.name=='PACA':\n",
    "            self.configs['drop_ratio']=0.1\n",
    "            self.configs['latent_dim']=math.ceil(self.X_train.shape[1]/2)\n",
    "            self.configs['in_features']=self.X_train.shape[1] #输入x的维度\n",
    "            self.configs['rand_seed']=self.configs['seed']\n",
    "            self.configs['eps']=1e-8    \n",
    "            self.configs['lr_scheduler']='fix'\n",
    "            self.configs['binary_data']=False\n",
    "            self.configs['weight_decay']=1e-5\n",
    "            self.configs['alpha']=2\n",
    "            self.configs['gamma']=10\n",
    "            self.configs['scheduler_warmup_epoch']=5\n",
    "            self.configs['scheduler_decay_epoch']=10\n",
    "            self.configs['scheduler_decay_rate']=1e-5 \n",
    "            self.configs['out_index']=-2\n",
    "        return self.configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91e67a79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T13:00:05.423393Z",
     "start_time": "2024-07-31T13:00:05.401794Z"
    },
    "code_folding": [
     2,
     13,
     22,
     38,
     47,
     53,
     67,
     69,
     71
    ]
   },
   "outputs": [],
   "source": [
    "seed_all(seed)\n",
    "device = torch.device('cuda')\n",
    "def FeatureSelect(X,p):\n",
    "    if p==1:\n",
    "        return X.toarray(),feature_names\n",
    "    else:\n",
    "        featurecount=int(X.shape[1]*p)\n",
    "        Selectfeatureindex=[x[0] for x in (sorted(enumerate(X.sum(axis=0).tolist()[0]),key=lambda x: x[1],reverse=True))][:featurecount]\n",
    "        Allfeatureindex=[i for i in range(X.shape[1])]\n",
    "        featureindex=[i for i in Allfeatureindex if i not in Selectfeatureindex]\n",
    "        new_x=np.delete(X.toarray(),featureindex,axis=1)\n",
    "        new_featurename=[feature_names[i] for i in Selectfeatureindex] \n",
    "        return new_x,new_featurename\n",
    "def LabelSelect(y):\n",
    "    b=[]\n",
    "    new_labelname=[i for i in label_names]\n",
    "    for i in range(y.shape[1]):\n",
    "        if y[:,i].sum()<=20:\n",
    "            b.append(i)\n",
    "            new_labelname.remove(label_names[i])\n",
    "    new_y=np.delete(y.toarray(),b,axis=1)\n",
    "    return new_y,new_labelname\n",
    "def macro_averaging_auc(Y, P, O):\n",
    "    n = (Y.shape[0] + O.shape[0]) // 2\n",
    "    l = (Y.shape[1] + O.shape[1]) // 2\n",
    "\n",
    "    p = np.zeros(l)\n",
    "    q = np.sum(Y, 0)\n",
    "\n",
    "    zero_column_count = np.sum(q == 0)\n",
    "#     print(f\"all zero for label: {zero_column_count}\")\n",
    "    r, c = np.nonzero(Y)\n",
    "    for i, j in zip(r, c):\n",
    "        p[j] += np.sum((Y[ : , j] < 0.5) * (O[ : , j] <= O[i, j]))\n",
    "\n",
    "    i = (q > 0) * (q < n)\n",
    "\n",
    "    return np.sum(p[i] / (q[i] * (n - q[i]))) / l\n",
    "def hamming_loss(Y, P, O):\n",
    "    n = (Y.shape[0] + P.shape[0]) // 2\n",
    "    l = (Y.shape[1] + P.shape[1]) // 2\n",
    "\n",
    "    s1 = np.sum(Y, 1)\n",
    "    s2 = np.sum(P, 1)\n",
    "    ss = np.sum(Y * P, 1)\n",
    "\n",
    "    return np.sum(s1 + s2 - 2 * ss) / (n * l)\n",
    "def one_error(Y, P, O):\n",
    "    n = (Y.shape[0] + O.shape[0]) // 2\n",
    "\n",
    "    i = np.argmax(O, 1)\n",
    "\n",
    "    return np.sum(1 - Y[range(n), i]) / n\n",
    "def ranking_loss(Y, P, O):\n",
    "    n = (Y.shape[0] + O.shape[0]) // 2\n",
    "    l = (Y.shape[1] + O.shape[1]) // 2\n",
    "\n",
    "    p = np.zeros(n)\n",
    "    q = np.sum(Y, 1)\n",
    "\n",
    "    r, c = np.nonzero(Y)\n",
    "    for i, j in zip(r, c): \n",
    "        p[i] += np.sum((Y[i, : ] < 0.5) * (O[i, : ] >= O[i, j]))\n",
    "\n",
    "    i = (q > 0) * (q < l)\n",
    "\n",
    "    return np.sum(p[i] / (q[i] * (l - q[i]))) / n\n",
    "def micro_f1(Y, P, O):\n",
    "    return f1_score(Y, P, average='micro')\n",
    "def macro_f1(Y, P, O):\n",
    "    return f1_score(Y, P, average='macro')\n",
    "def eval_metrics(mod, metrics, datasets, idx,batch_size,device):\n",
    "    res_dict = {}\n",
    "    mod.eval()\n",
    "    y_true_list = []\n",
    "    y_scores_list = []\n",
    "    test_dataloader = DataLoader(datasets, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    for x, y in test_dataloader:\n",
    "        _, y_pred=mod.predict(x)\n",
    "#         pdb.set_trace()\n",
    "        y_true_list.append(y.cpu().numpy())\n",
    "        y_scores_list.append(y_pred.cpu().numpy())\n",
    "    y_true = np.vstack(y_true_list)\n",
    "    y_prob = np.vstack(y_scores_list)\n",
    "    y_pred = np.round(y_prob).astype(int)\n",
    "    res_dict1 = {metric.__name__: metric(y_true, y_pred,y_prob) for metric in metrics}\n",
    "#         # Calculate metric.\n",
    "#         res_dict1 = {metric.__name__: metric(y_true, y_pred) for metric in metrics[:2]}\n",
    "#         res_dict2 = {metric.__name__: metric(y_true, y_prob) for metric in metrics[2:5]}\n",
    "#         res_dict1.update(res_dict2)\n",
    "#     res_dict[f'dataset_{ix}']=res_dict1\n",
    "#         res_dict[f'dataset_{ix}'] = {metric.__name__: metric(y_true, y_pred) for metric in metrics[:8]}\n",
    "#         res_dict[f'dataset_{ix}'] = {metric.__name__: metric(y_true, y_prob) for metric in metrics[8:9]}\n",
    "    return res_dict1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1e0ff6",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f40dd16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:13:03.952053Z",
     "start_time": "2024-07-02T08:09:11.664359Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    " def training(configs,test_epoch):\n",
    "    net=CLIFModel(configs).to(device)\n",
    "    num_epochs =configs['epoch']\n",
    "    batch_size = configs['batch_size']\n",
    "    lr = configs['lr']\n",
    "    label_dim=configs['num_classes']\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    writer = SummaryWriter(comment=f'{dataname}')\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    validation_dataset, test_dataset_new = train_test_split(test_dataset, test_size=0.5, random_state=42)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    best_auc = 0\n",
    "    best_epoch=0\n",
    "    best_model_state = None\n",
    "    epoch_losses_train = []\n",
    "    batch_losses = []\n",
    "    epoch_losses_val= []\n",
    "    all_loss_list=[]\n",
    "    total_minority_samples = len(configs['min_ins_idx'])\n",
    "    \n",
    "    warmup_epochs =5\n",
    "    total_steps = num_epochs * int(configs['num_ins']/batch_size)\n",
    "    warmup_steps = warmup_epochs * int(configs['num_ins']/batch_size)\n",
    "    global_step = 0\n",
    "    def update_learning_rate(optimizer, global_step, warmup_steps=warmup_steps, base_lr=lr):\n",
    "        if global_step < warmup_steps:\n",
    "            lr = base_lr * (global_step / warmup_steps)\n",
    "        else:\n",
    "            lr = base_lr\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr \n",
    "     \n",
    "    for epoch in range(num_epochs+1): \n",
    "        net.train()\n",
    "        loss_tracker = 0.0\n",
    "        all_indices = [] \n",
    "        batch_counter = 0       \n",
    "        for x, y in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(x)\n",
    "            loss_dict = net.loss_function_train(outputs, y)\n",
    "            loss = loss_dict['Loss']\n",
    "            individual_losses =net._loss_per_label(outputs[configs['out_index']], y)\n",
    "            if epoch==test_epoch:\n",
    "                for i in range(individual_losses.shape[0]):\n",
    "                    all_loss_list.append(round(individual_losses[i].item(), 3))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            update_learning_rate(optimizer, global_step) \n",
    "            loss_tracker += loss.item()\n",
    "            batch_counter += 1\n",
    "\n",
    "        writer.add_scalar('train/loss', loss_tracker, epoch)\n",
    "        epoch_losses_train.append(loss_tracker /batch_counter)  \n",
    "        \n",
    "        \n",
    "        \n",
    "        net.eval()\n",
    "        y_true_list = []\n",
    "        y_scores_list = []\n",
    "        loss_tracker = 0.0\n",
    "        batch_counter = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in validation_dataloader:\n",
    "                _, y_pred=net.predict(x)\n",
    "                y_true_list.append(y.cpu().numpy())\n",
    "                y_scores_list.append(y_pred.cpu().numpy())\n",
    "                \n",
    "                outputs = net(x)\n",
    "                loss_dict = net.loss_function_train(outputs, y)\n",
    "                loss = loss_dict['Loss']\n",
    "                loss_tracker += loss.item()\n",
    "                batch_counter += 1\n",
    "        epoch_losses_val.append(loss_tracker /batch_counter)\n",
    "        y_true = np.vstack(y_true_list)\n",
    "        y_scores = np.vstack(y_scores_list)\n",
    "        auc = macro_averaging_auc(y_true,y_scores, y_scores)\n",
    "        writer.add_scalar('val/auc', auc, epoch)\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_epoch=epoch\n",
    "            best_model_state = net.state_dict().copy()\n",
    "    net.load_state_dict(best_model_state)\n",
    "    mets = eval_metrics(net, [macro_f1, micro_f1, macro_averaging_auc, ranking_loss, hamming_loss, one_error], test_dataset_new, configs['out_index'],configs['batch_size'],torch.device('cuda'))\n",
    "    return all_loss_list\n",
    "\n",
    "# path_to_arff_files = [\"Corel5k\",\"bibtex\"]\n",
    "# label_counts = [374,159]\n",
    "# select_feature=[1,1]\n",
    "# sample_loss_dict={}\n",
    "\n",
    "path_to_arff_files = [\"yahoo-Business1\"]\n",
    "label_counts = [28]\n",
    "select_feature=[0.02]\n",
    "sample_loss_dict={}\n",
    "\n",
    "for idx, dataname in enumerate(path_to_arff_files):\n",
    "    path_to_arff_file = f\"/home/tt/{dataname}.arff\"\n",
    "    X, y, feature_names, label_names = load_from_arff(\n",
    "    path_to_arff_file,\n",
    "    label_count=label_counts[idx],\n",
    "    label_location=\"end\",\n",
    "    load_sparse=False,\n",
    "    return_attribute_definitions=True\n",
    "    )\n",
    "    X,feature_names=FeatureSelect(X,select_feature[idx])  \n",
    "    y,label_names=LabelSelect(y)\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    k_fold = IterativeStratification(n_splits=5,order=1,random_state=42)\n",
    "    dicts=[]\n",
    "    for idx,(train,test) in enumerate(k_fold.split(X,y)):\n",
    "        if idx==4:\n",
    "            train_dataset = TensorDataset(torch.tensor(X[train], device=device, dtype=torch.float),torch.tensor(y[train], device=device,dtype=torch.float))\n",
    "            test_dataset = TensorDataset(torch.tensor(X[test], device=device, dtype=torch.float), torch.tensor(y[test], device=device, dtype=torch.float))    \n",
    "            configs=CFG('CLIF',X[train],y[train]).getconfig()\n",
    "            result=training(configs,70)\n",
    "            sample_loss_dict[dataname]=result\n",
    "#             epoch_loss_dict_train[dataname]=epoch_losses_train\n",
    "#             epoch_loss_dict_val[dataname]=epoch_losses_val\n",
    "#             epoch_auc_dict_val[dataname]=epoch_auc_val\n",
    "#             print(best_epoch)\n",
    "\n",
    "import pickle\n",
    "# with open('epoch_loss_dict_train0.pickle', 'wb') as f:\n",
    "#     pickle.dump(epoch_loss_dict_train, f)\n",
    "# with open('epoch_loss_dict_val0.pickle', 'wb') as f:\n",
    "#     pickle.dump(epoch_loss_dict_val, f)\n",
    "# with open('epoch_auc_dict_val0.pickle', 'wb') as f:\n",
    "#     pickle.dump(epoch_auc_dict_val, f)\n",
    "# with open('batch_loss_dict_train1.pickle', 'wb') as f:\n",
    "#     pickle.dump(batch_loss_dict_train, f)\n",
    "with open('Figure/sample_loss_dict_Base(70).pickle', 'wb') as f:\n",
    "    pickle.dump(sample_loss_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5526a3d9",
   "metadata": {},
   "source": [
    "# Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9041437",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:39:57.772099Z",
     "start_time": "2024-07-02T08:36:10.681146Z"
    },
    "code_folding": [
     0,
     9
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yahoo-Business1\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "class StratifiedBatchSampler:\n",
    "    def __init__(self, labels, batch_size):\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(labels))\n",
    "        self.unique_labels, self.label_counts = np.unique(labels, axis=0, return_counts=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch_indices = []\n",
    "        label_indices = {tuple(label): np.where((self.labels == label).all(axis=1))[0].tolist() for label in self.unique_labels}\n",
    "        \n",
    "        while len(self.indices) >= self.batch_size:\n",
    "            for label, count in zip(self.unique_labels, self.label_counts):\n",
    "                if len(batch_indices) >= self.batch_size:\n",
    "                    break\n",
    "                label_tuple = tuple(label)\n",
    "                if len(label_indices[label_tuple]) > 0:\n",
    "                    chosen_indices = np.random.choice(label_indices[label_tuple], size=min(count, self.batch_size - len(batch_indices)), replace=False)\n",
    "                    batch_indices.extend(chosen_indices)\n",
    "                    label_indices[label_tuple] = [i for i in label_indices[label_tuple] if i not in chosen_indices]\n",
    "\n",
    "            if len(batch_indices) == self.batch_size:\n",
    "                yield batch_indices\n",
    "                batch_indices = []\n",
    "\n",
    "        if len(batch_indices) > 0:\n",
    "            yield batch_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices) // self.batch_size\n",
    "\n",
    "class CustomDataloader:\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = torch.arange(len(dataset), dtype=torch.long)  # 确保索引为长整型\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        if self.shuffle:\n",
    "            self.indices = torch.randperm(len(self.dataset), dtype=torch.long)  # 确保索引为长整型\n",
    "\n",
    "    def __iter__(self):\n",
    "        sampler = torch.utils.data.BatchSampler(\n",
    "            torch.utils.data.SubsetRandomSampler(self.indices),\n",
    "            batch_size=self.batch_size,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "        for batch_indices in sampler:\n",
    "            yield self.get_data_from_indices(batch_indices)\n",
    "\n",
    "    def get_data_from_indices(self, indices):\n",
    "        x, y = zip(*[self.dataset[i] for i in indices])\n",
    "        return indices, torch.stack(x), torch.stack(y)\n",
    "\n",
    "# 示例训练函数\n",
    "def training(configs, test_epoch):\n",
    "    net = CLIFModel(configs).to(device)\n",
    "    num_epochs = configs['epoch']\n",
    "    batch_size = configs['batch_size']\n",
    "    lr = 1e-4\n",
    "    label_dim = configs['num_classes']\n",
    "    weight_decay = 1e-4\n",
    "    betas = (0.9, 0.999)\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "    writer = SummaryWriter(comment=f'{dataname}')\n",
    "\n",
    "    sample_probabilities = {}\n",
    "    para_loss = {key: 0 for key in range(configs['num_ins'])}\n",
    "    custom_dataloader = CustomDataloader(train_dataset, batch_size=batch_size)\n",
    "    validation_dataset, test_dataset_new = train_test_split(test_dataset, test_size=0.5, random_state=42)\n",
    "    test_dataloader = DataLoader(test_dataset_new, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    best_auc = 0\n",
    "    best_model_state = None\n",
    "    epoch_losses_train = []\n",
    "    all_loss_list = []\n",
    "\n",
    "    warmup_epochs = 5\n",
    "    total_steps = num_epochs * int(configs['num_ins'] / batch_size)\n",
    "    warmup_steps = warmup_epochs * int(configs['num_ins'] / batch_size)\n",
    "    global_step = 0\n",
    "\n",
    "    def update_learning_rate(optimizer, global_step, warmup_steps=warmup_steps, base_lr=lr):\n",
    "        if global_step < warmup_steps:\n",
    "            lr = base_lr * (global_step / warmup_steps)\n",
    "        else:\n",
    "            lr = base_lr\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    for epoch in range(num_epochs + 1):\n",
    "        net.train()\n",
    "        batch_counter = 0\n",
    "        loss_tracker = 0.0\n",
    "        loss_tracker2 = 0.0\n",
    "        all_individual_losses = {}\n",
    "        custom_dataloader.set_epoch(epoch)\n",
    "        for idx, x, y in custom_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(x)\n",
    "            loss_dict = net.loss_function_train(outputs, y)\n",
    "            loss = loss_dict['Loss']\n",
    "            individual_losses1 = net._loss_per_label(outputs[configs['out_index']], y)\n",
    "            if epoch == test_epoch:\n",
    "                for i in range(individual_losses1.shape[0]):\n",
    "                    all_loss_list.append(round(individual_losses1[i].item(), 3))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            update_learning_rate(optimizer, global_step)\n",
    "            batch_counter += 1\n",
    "        writer.add_scalar('train/loss', loss_tracker, epoch)\n",
    "        epoch_losses_train.append(loss_tracker / batch_counter)\n",
    "\n",
    "        net.eval()\n",
    "        y_true_list = []\n",
    "        y_scores_list = []\n",
    "        loss_tracker = 0.0\n",
    "        batch_counter = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in validation_dataloader:\n",
    "                _, y_pred = net.predict(x)\n",
    "                y_true_list.append(y.cpu().numpy())\n",
    "                y_scores_list.append(y_pred.cpu().numpy())\n",
    "\n",
    "                outputs = net(x)\n",
    "                loss_dict = net.loss_function_train(outputs, y)\n",
    "                loss = loss_dict['Loss']\n",
    "                loss_tracker += loss.item()\n",
    "                batch_counter += 1\n",
    "        y_true = np.vstack(y_true_list)\n",
    "        y_scores = np.vstack(y_scores_list)\n",
    "\n",
    "        auc = macro_averaging_auc(y_true, y_scores, y_scores)\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_epoch = epoch\n",
    "            best_model_state = net.state_dict().copy()\n",
    "    net.load_state_dict(best_model_state)\n",
    "    mets = eval_metrics(net, [macro_f1, micro_f1, macro_averaging_auc, ranking_loss, hamming_loss, one_error], test_dataset_new, configs['out_index'], configs['batch_size'], torch.device('cuda'))\n",
    "\n",
    "    return all_loss_list\n",
    "\n",
    "path_to_arff_files = [\"emotions\",\"scene\",\"yeast\", \"Corel5k\",\"rcv1subset1\",\"rcv1subset2\",\"rcv1subset3\",\"yahoo-Business1\",\"yahoo-Arts1\",\"bibtex\",'tmc2007','enron','cal500','LLOG-F']\n",
    "label_counts = [6, 6,14,374,101,101,101,28,25,159,22,53,174,75]\n",
    "select_feature=[1,1,1,1,0.02,0.02,0.02,0.05,0.05,1,0.01,1,1,1]\n",
    "\n",
    "path_to_arff_files = [\"Corel5k\",\"bibtex\"]\n",
    "label_counts = [374,159]\n",
    "select_feature=[1,1]\n",
    "sample_loss_dict={}\n",
    "\n",
    "path_to_arff_files = [\"yahoo-Business1\"]\n",
    "label_counts = [28]\n",
    "select_feature=[0.02]\n",
    "sample_loss_dict={}\n",
    "\n",
    "for idx, dataname in enumerate(path_to_arff_files):\n",
    "    path_to_arff_file = f\"/home/tt/{dataname}.arff\"\n",
    "    X, y, feature_names, label_names = load_from_arff(\n",
    "    path_to_arff_file,\n",
    "    label_count=label_counts[idx],\n",
    "    label_location=\"end\",\n",
    "    load_sparse=False,\n",
    "    return_attribute_definitions=True\n",
    "    )\n",
    "    X,feature_names=FeatureSelect(X,select_feature[idx])  \n",
    "    y,label_names=LabelSelect(y)\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    warm_epoch=3\n",
    "    print(dataname)\n",
    "    k_fold = IterativeStratification(n_splits=5,order=1,random_state=42)\n",
    "    dicts=[]\n",
    "    for idx,(train,test) in enumerate(k_fold.split(X,y)):\n",
    "        if idx==4:\n",
    "            train_dataset = TensorDataset(torch.tensor(X[train], device=device, dtype=torch.float),torch.tensor(y[train], device=device,dtype=torch.float))\n",
    "            test_dataset = TensorDataset(torch.tensor(X[test], device=device, dtype=torch.float), torch.tensor(y[test], device=device, dtype=torch.float))    \n",
    "            configs=CFG('CLIF',X[train],y[train]).getconfig()\n",
    "            result=training(configs,70)\n",
    "            sample_loss_dict[dataname]=result\n",
    "#             epoch_loss_dict_train[dataname]=epoch_losses_train\n",
    "#             epoch_loss_dict_val[dataname]=epoch_losses_val\n",
    "#             epoch_auc_dict_val[dataname]=epoch_auc_val\n",
    "#             print(best_epoch)\n",
    "# with open('epoch_loss_dict_train0.pickle', 'wb') as f:\n",
    "#     pickle.dump(epoch_loss_dict_train, f)\n",
    "# with open('epoch_loss_dict_val0.pickle', 'wb') as f:\n",
    "#     pickle.dump(epoch_loss_dict_val, f)\n",
    "# with open('epoch_auc_dict_val0.pickle', 'wb') as f:\n",
    "#     pickle.dump(epoch_auc_dict_val, f)\n",
    "# with open('batch_loss_dict_train1.pickle', 'wb') as f:\n",
    "#     pickle.dump(batch_loss_dict_train, f)\n",
    "with open('Figure/sample_loss_dict_Balance(70).pickle', 'wb') as f:\n",
    "    pickle.dump(sample_loss_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6a579e",
   "metadata": {},
   "source": [
    "# Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c178e4e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T08:31:06.678389Z",
     "start_time": "2024-07-02T08:26:47.004134Z"
    },
    "code_folding": [
     0,
     49
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yahoo-Business1\n"
     ]
    }
   ],
   "source": [
    "def calculate_probabilities(losses_dict,se):\n",
    "    losses_np = np.array(list(losses_dict.values()))\n",
    "    Delta=max(losses_np)/len(losses_np)\n",
    "    se = se \n",
    "    N = len(losses_np)\n",
    "    quantized_indices = np.ceil(losses_np / Delta).astype(int)\n",
    "    probabilities = (np.exp(np.log(se) / N)) ** quantized_indices\n",
    "    probabilities_dict = {key: prob for key, prob in zip(losses_dict.keys(), probabilities)}\n",
    "    \n",
    "    return probabilities_dict\n",
    "class CustomDataLoader:\n",
    "    def __init__(self, dataset, net, configs,sample_probabilities,warm_epoch):\n",
    "        self.shuffle = False\n",
    "        self.dataset = dataset\n",
    "        self.net = net\n",
    "        self.batch_size = configs['batch_size']\n",
    "        self.sample_probabilities =sample_probabilities\n",
    "        self.dataset_indices=list(range(len(self.dataset)))\n",
    "        self.warm_epoch=warm_epoch       \n",
    "    def __iter__(self):\n",
    "        if self.current_epoch < self.warm_epoch:\n",
    "            batch_counter = 0\n",
    "            for start_idx in range(0, len(self.dataset_indices), self.batch_size):\n",
    "                end_idx = min(start_idx + self.batch_size, len(self.dataset_indices))\n",
    "                batch_indices = self.dataset_indices[start_idx:end_idx]\n",
    "                batch_counter += 1\n",
    "                yield self.get_data_from_indices(batch_indices)\n",
    "        else:\n",
    "            all_indices = list(self.sample_probabilities.keys())        \n",
    "            all_probabilities = np.array([self.sample_probabilities[idx] for idx in all_indices])\n",
    "            non_zero_mask = all_probabilities != 0\n",
    "            non_zero_sum = all_probabilities[non_zero_mask].sum()\n",
    "            all_probabilities[non_zero_mask] /= non_zero_sum\n",
    "            total_samples = len(self.dataset)\n",
    "            num_batches = np.ceil(total_samples / self.batch_size).astype(int)\n",
    "\n",
    "            for _ in range(num_batches):\n",
    "                num_required = self.batch_size \n",
    "                if num_required > 0:\n",
    "                    chosen_indices = np.random.choice(all_indices, size=num_required, replace=False, p=all_probabilities)\n",
    "                else:\n",
    "                    chosen_indices = []\n",
    "                batch_indices = list(chosen_indices)\n",
    "                yield self.get_data_from_indices(batch_indices)\n",
    "    def get_data_from_indices(self, indices):\n",
    "        x, y = zip(*[self.dataset[i] for i in indices])\n",
    "        return indices, torch.stack(x), torch.stack(y)\n",
    "    def set_epoch(self, epoch):\n",
    "        self.current_epoch = epoch  \n",
    "def training(configs,test_epoch):\n",
    "    net=CLIFModel(configs).to(device)\n",
    "    num_epochs =configs['epoch']\n",
    "    batch_size = configs['batch_size']\n",
    "    lr = 1e-4\n",
    "    label_dim=configs['num_classes']\n",
    "    weight_decay = 1e-4  \n",
    "    betas = (0.9, 0.999) \n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr,  \n",
    "                           betas=betas, weight_decay=weight_decay)\n",
    "    writer = SummaryWriter(comment=f'{dataname}')\n",
    "\n",
    "    sample_probabilities = {}\n",
    "    para_loss={key: 0 for key in range(configs['num_ins'])}\n",
    "    custom_dataloader = CustomDataLoader(train_dataset, net=net, configs=configs,sample_probabilities=sample_probabilities,warm_epoch=warm_epoch)\n",
    "    validation_dataset, test_dataset_new = train_test_split(test_dataset, test_size=0.5, random_state=42)   \n",
    "    test_dataloader = DataLoader(test_dataset_new, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    best_auc = 0\n",
    "    best_model_state = None\n",
    "    epoch_losses_train = []\n",
    "    all_loss_list=[]\n",
    "\n",
    "    warmup_epochs =5\n",
    "    total_steps = num_epochs * int(configs['num_ins']/batch_size)\n",
    "    warmup_steps = warmup_epochs * int(configs['num_ins']/batch_size)\n",
    "    global_step = 0\n",
    "    def update_learning_rate(optimizer, global_step, warmup_steps=warmup_steps, base_lr=lr):\n",
    "        if global_step < warmup_steps:\n",
    "            lr = base_lr * (global_step / warmup_steps)\n",
    "        else:\n",
    "            lr = base_lr\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr         \n",
    "    for epoch in range(num_epochs+1): \n",
    "        net.train()\n",
    "        batch_counter = 0   \n",
    "        loss_tracker = 0.0\n",
    "        loss_tracker2=0.0\n",
    "        all_individual_losses = {}\n",
    "        custom_dataloader.set_epoch(epoch) \n",
    "        for idx, x, y in custom_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(x)\n",
    "            loss_dict = net.loss_function_train(outputs, y)\n",
    "            loss = loss_dict['Loss']\n",
    "            individual_losses =net.custom_multilabel_soft_margin_loss(outputs[configs['out_index']], y,  [configs['weight_list'][i] for i in idx])\n",
    "            \n",
    "            individual_losses1 =net._loss_per_label(outputs[configs['out_index']], y)\n",
    "            if epoch==test_epoch:\n",
    "                for i in range(individual_losses1.shape[0]):\n",
    "                    all_loss_list.append(round(individual_losses1[i].item(), 3))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            update_learning_rate(optimizer, global_step)\n",
    "\n",
    "            for i, prob in zip(idx, individual_losses):\n",
    "                para_loss[i]=prob.item()\n",
    "            current_probabilities = calculate_probabilities(para_loss,8)\n",
    "            for i in idx:\n",
    "                if i in current_probabilities:\n",
    "                    sample_probabilities[i] = current_probabilities[i]\n",
    "\n",
    "            custom_dataloader.sample_probabilities = sample_probabilities\n",
    "            batch_counter+=1\n",
    "        writer.add_scalar('train/loss', loss_tracker, epoch)\n",
    "        epoch_losses_train.append(loss_tracker /batch_counter)\n",
    "\n",
    "        net.eval()\n",
    "        y_true_list = []\n",
    "        y_scores_list = []\n",
    "        loss_tracker = 0.0\n",
    "        batch_counter = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in validation_dataloader:\n",
    "                _, y_pred=net.predict(x)\n",
    "                y_true_list.append(y.cpu().numpy())\n",
    "                y_scores_list.append(y_pred.cpu().numpy())\n",
    "                \n",
    "                outputs = net(x)\n",
    "                loss_dict = net.loss_function_train(outputs, y)\n",
    "                loss = loss_dict['Loss']\n",
    "                loss_tracker += loss.item()\n",
    "                batch_counter += 1\n",
    "        y_true = np.vstack(y_true_list)\n",
    "        y_scores = np.vstack(y_scores_list)\n",
    "\n",
    "        auc = macro_averaging_auc(y_true,y_scores, y_scores)\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_epoch=epoch\n",
    "            best_model_state = net.state_dict().copy()\n",
    "    net.load_state_dict(best_model_state)\n",
    "    mets = eval_metrics(net, [macro_f1, micro_f1, macro_averaging_auc, ranking_loss, hamming_loss, one_error], test_dataset_new, configs['out_index'],configs['batch_size'],torch.device('cuda'))\n",
    "\n",
    "    return all_loss_list\n",
    "\n",
    "\n",
    "path_to_arff_files = [\"emotions\",\"scene\",\"yeast\", \"Corel5k\",\"rcv1subset1\",\"rcv1subset2\",\"rcv1subset3\",\"yahoo-Business1\",\"yahoo-Arts1\",\"bibtex\",'tmc2007','enron','cal500','LLOG-F']\n",
    "label_counts = [6, 6,14,374,101,101,101,28,25,159,22,53,174,75]\n",
    "select_feature=[1,1,1,1,0.02,0.02,0.02,0.05,0.05,1,0.01,1,1,1]\n",
    "\n",
    "path_to_arff_files = [\"Corel5k\",\"bibtex\"]\n",
    "label_counts = [374,159]\n",
    "select_feature=[1,1]\n",
    "sample_loss_dict={}\n",
    "\n",
    "path_to_arff_files = [\"yahoo-Business1\"]\n",
    "label_counts = [28]\n",
    "select_feature=[0.02]\n",
    "sample_loss_dict={}\n",
    "\n",
    "for idx, dataname in enumerate(path_to_arff_files):\n",
    "    path_to_arff_file = f\"/home/tt/{dataname}.arff\"\n",
    "    X, y, feature_names, label_names = load_from_arff(\n",
    "    path_to_arff_file,\n",
    "    label_count=label_counts[idx],\n",
    "    label_location=\"end\",\n",
    "    load_sparse=False,\n",
    "    return_attribute_definitions=True\n",
    "    )\n",
    "    X,feature_names=FeatureSelect(X,select_feature[idx])  \n",
    "    y,label_names=LabelSelect(y)\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    warm_epoch=3\n",
    "    print(dataname)\n",
    "    k_fold = IterativeStratification(n_splits=5,order=1,random_state=42)\n",
    "    dicts=[]\n",
    "    for idx,(train,test) in enumerate(k_fold.split(X,y)):\n",
    "        if idx==4:\n",
    "            train_dataset = TensorDataset(torch.tensor(X[train], device=device, dtype=torch.float),torch.tensor(y[train], device=device,dtype=torch.float))\n",
    "            test_dataset = TensorDataset(torch.tensor(X[test], device=device, dtype=torch.float), torch.tensor(y[test], device=device, dtype=torch.float))    \n",
    "            configs=CFG('CLIF',X[train],y[train]).getconfig()\n",
    "            result=training(configs,70)\n",
    "            sample_loss_dict[dataname]=result\n",
    "#             epoch_loss_dict_train[dataname]=epoch_losses_train\n",
    "#             epoch_loss_dict_val[dataname]=epoch_losses_val\n",
    "#             epoch_auc_dict_val[dataname]=epoch_auc_val\n",
    "#             print(best_epoch)\n",
    "# with open('epoch_loss_dict_train0.pickle', 'wb') as f:\n",
    "#     pickle.dump(epoch_loss_dict_train, f)\n",
    "# with open('epoch_loss_dict_val0.pickle', 'wb') as f:\n",
    "#     pickle.dump(epoch_loss_dict_val, f)\n",
    "# with open('epoch_auc_dict_val0.pickle', 'wb') as f:\n",
    "#     pickle.dump(epoch_auc_dict_val, f)\n",
    "# with open('batch_loss_dict_train1.pickle', 'wb') as f:\n",
    "#     pickle.dump(batch_loss_dict_train, f)\n",
    "# with open('Figure/sample_loss_dict_Hard(70).pickle', 'wb') as f:\n",
    "#     pickle.dump(sample_loss_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151b84fa",
   "metadata": {},
   "source": [
    "# Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe0d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_H(H):\n",
    "    var_H = np.var(H, ddof=1)  # ddof=1 for sample variance\n",
    "    std = np.sqrt(var_H + var_H**2 / (len(H) - 1))\n",
    "    return std\n",
    "def update_H(H, y_pred, ids, max_history_length=epoch):\n",
    "    y_pred_numpy = y_pred.detach().cpu().numpy()\n",
    "    for i, idx in enumerate(ids):\n",
    "        if idx not in H:\n",
    "            H[idx] = deque(maxlen=max_history_length) \n",
    "        H[idx].append(y_pred_numpy[i])  \n",
    "    return H\n",
    "def update_E(H,E,ids,label_dim):\n",
    "    for idx in ids:\n",
    "        current_predictions_history = np.array(H[idx])\n",
    "        for j in range(label_dim):      \n",
    "            E[idx,j]=std_H(current_predictions_history[:, j])\n",
    "    return E\n",
    "\n",
    "def update_U(E, U, epoch):\n",
    "    U = np.sum(E, axis=1)\n",
    "    return U\n",
    "\n",
    "def calculate_probabilities(U, epoch, e_end, e0, se0):\n",
    "    N=len(U)\n",
    "    Delta = 1 / N \n",
    "    # Exponentially decay the selection pressure as training progresses\n",
    "    se = se0 * (np.exp(np.log(1 / se0) / (e_end - e0)))**(epoch - e0)\n",
    "    U_min = np.min(U)\n",
    "    U_max = np.max(U)\n",
    "    U_normalized = (U - U_min) / (U_max - U_min)\n",
    "    quantized_indices = np.ceil((1 - U_normalized) / Delta).astype(int)\n",
    "#     pdb.set_trace()\n",
    "    # Calculate the exponent for the sampling probability\n",
    "    exponent = -np.log(se) / N * quantized_indices \n",
    "    # Calculate the unnormalized probabilities\n",
    "    unnormalized_probabilities = np.exp(exponent)\n",
    "    return unnormalized_probabilities\n",
    "\n",
    "class CustomDataLoader:\n",
    "    def __init__(self, dataset, net, configs,H,E,U,sample_probabilities,warm_epoch):\n",
    "        self.shuffle = False\n",
    "        self.dataset = dataset\n",
    "        self.net = net\n",
    "        self.batch_size = configs['batch_size']\n",
    "        self.sample_probabilities =sample_probabilities\n",
    "        self.dataset_indices=list(range(len(self.dataset)))\n",
    "        self.warm_epoch=warm_epoch   \n",
    "        \n",
    "        self.H=H\n",
    "        self.E=E\n",
    "        self.U=U    \n",
    "                              \n",
    "    def __iter__(self):\n",
    "        if self.current_epoch < self.warm_epoch:\n",
    "            batch_counter = 0\n",
    "            for start_idx in range(0, len(self.dataset_indices), self.batch_size):\n",
    "                end_idx = min(start_idx + self.batch_size, len(self.dataset_indices))\n",
    "                batch_indices = self.dataset_indices[start_idx:end_idx]\n",
    "                batch_counter += 1\n",
    "                yield self.get_data_from_indices(batch_indices)\n",
    "        else:\n",
    "            all_indices = self.dataset_indices  \n",
    "            all_probabilities = self.sample_probabilities   \n",
    "\n",
    "            total_sum = all_probabilities.sum()\n",
    "            all_probabilities /= total_sum\n",
    "\n",
    "            total_samples = len(self.dataset)\n",
    "            num_batches = np.ceil(total_samples / self.batch_size).astype(int)\n",
    "\n",
    "            for _ in range(num_batches):\n",
    "                num_required = self.batch_size \n",
    "                if num_required > 0:\n",
    "                    chosen_indices = np.random.choice(all_indices, size=num_required, replace=False, p=all_probabilities)\n",
    "                else:\n",
    "                    chosen_indices = []\n",
    "                batch_indices = list(chosen_indices)\n",
    "                yield self.get_data_from_indices(batch_indices)\n",
    "                \n",
    "    def get_data_from_indices(self, indices):\n",
    "        x, y = zip(*[self.dataset[i] for i in indices])\n",
    "        return indices, torch.stack(x), torch.stack(y)\n",
    "    def set_epoch(self, epoch):\n",
    "        self.current_epoch = epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f401f",
   "metadata": {},
   "source": [
    "# Recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7959b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(H_ij_t):\n",
    "    classes = [0, 1]\n",
    "    T = len(H_ij_t)\n",
    "    probabilities = []\n",
    "#     class 1\n",
    "    count_1 = sum([1 for y in H_ij_t if y > 0.5])\n",
    "    P_1 = count_c / T\n",
    "    probabilities.append(P_1)\n",
    "#    class 0\n",
    "    P_0= 1-P_1\n",
    "    probabilities.append(P_0)\n",
    "    entropy_value = -sum(P_c * np.log2(P_c) for P_c in probabilities if P_c > 0)  \n",
    "    return entropy_value\n",
    "# 队列函数\n",
    "def update_H(H, y_pred, ids, max_history_length=5):\n",
    "# y_pred_numpy[batch*q]   批样本ids在原训练集的id\n",
    "    y_pred_numpy = y_pred.detach().cpu().numpy()\n",
    "    for i, idx in enumerate(ids):\n",
    "        if idx not in H:\n",
    "            H[idx] = deque(maxlen=max_history_length) \n",
    "        H[idx].append(y_pred_numpy[i])  \n",
    "    return H\n",
    "# 每个标签不确定性度量的中间函数\n",
    "def update_E(H,E,ids,label_dim):\n",
    "    for idx in ids:\n",
    "# current_predictions_history[max_history_length*q]\n",
    "        current_predictions_history = np.array(H[idx])\n",
    "        for j in range(label_dim):      \n",
    "            E[idx,j]=entropy(current_predictions_history[:, j])\n",
    "    return E\n",
    "\n",
    "def update_U(E, U, epoch):\n",
    "    U = np.sum(E, axis=1)\n",
    "    return U\n",
    "\n",
    "def calculate_probabilities(U, epoch, e_end, e0, se0):\n",
    "    N=len(U)\n",
    "    Delta = 1 / N \n",
    "    # Exponentially decay the selection pressure as training progresses\n",
    "    se = se0 * (np.exp(np.log(1 / se0) / (e_end - e0)))**(epoch - e0)\n",
    "    U_min = np.min(U)\n",
    "    U_max = np.max(U)\n",
    "    U_normalized = (U - U_min) / (U_max - U_min)\n",
    "    quantized_indices = np.ceil((1 - U_normalized) / Delta).astype(int)\n",
    "#     pdb.set_trace()\n",
    "    # Calculate the exponent for the sampling probability\n",
    "    exponent = -np.log(se) / N * quantized_indices \n",
    "    # Calculate the unnormalized probabilities\n",
    "    unnormalized_probabilities = np.exp(exponent)\n",
    "    return unnormalized_probabilities\n",
    "\n",
    "class CustomDataLoader:\n",
    "    def __init__(self, dataset, net, configs,H,E,U,sample_probabilities,warm_epoch):\n",
    "        self.shuffle = False\n",
    "        self.dataset = dataset\n",
    "        self.net = net\n",
    "        self.batch_size = configs['batch_size']\n",
    "        self.sample_probabilities =sample_probabilities\n",
    "        self.dataset_indices=list(range(len(self.dataset)))\n",
    "        self.warm_epoch=warm_epoch   \n",
    "        \n",
    "        self.H=H\n",
    "        self.E=E\n",
    "        self.U=U    \n",
    "                              \n",
    "    def __iter__(self):\n",
    "        if self.current_epoch < self.warm_epoch:\n",
    "            batch_counter = 0\n",
    "            for start_idx in range(0, len(self.dataset_indices), self.batch_size):\n",
    "                end_idx = min(start_idx + self.batch_size, len(self.dataset_indices))\n",
    "                batch_indices = self.dataset_indices[start_idx:end_idx]\n",
    "                batch_counter += 1\n",
    "                yield self.get_data_from_indices(batch_indices)\n",
    "        else:\n",
    "            all_indices = self.dataset_indices  \n",
    "            all_probabilities = self.sample_probabilities   \n",
    "\n",
    "            total_sum = all_probabilities.sum()\n",
    "            all_probabilities /= total_sum\n",
    "\n",
    "            total_samples = len(self.dataset)\n",
    "            num_batches = np.ceil(total_samples / self.batch_size).astype(int)\n",
    "\n",
    "            for _ in range(num_batches):\n",
    "                num_required = self.batch_size \n",
    "                if num_required > 0:\n",
    "                    chosen_indices = np.random.choice(all_indices, size=num_required, replace=False, p=all_probabilities)\n",
    "                else:\n",
    "                    chosen_indices = []\n",
    "                batch_indices = list(chosen_indices)\n",
    "                yield self.get_data_from_indices(batch_indices)\n",
    "                \n",
    "    def get_data_from_indices(self, indices):\n",
    "        x, y = zip(*[self.dataset[i] for i in indices])\n",
    "        return indices, torch.stack(x), torch.stack(y)\n",
    "    def set_epoch(self, epoch):\n",
    "        self.current_epoch = epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341fe2f",
   "metadata": {},
   "source": [
    "# Uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4b0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_H(H, y_pred, ids, max_history_length=5):\n",
    "    y_pred_numpy = y_pred.detach().cpu().numpy()\n",
    "    for i, idx in enumerate(ids):\n",
    "        if idx not in H:\n",
    " # 为每个样本创建一个新的deque，限制最大长度\n",
    "            H[idx] = deque(maxlen=max_history_length) \n",
    "# 添加新的预测，如果deque已满，最早的预测将自动被移除\n",
    "        H[idx].append(y_pred_numpy[i])  \n",
    "    return H\n",
    "def update_E(H,E,ids,label_dim):\n",
    "    \n",
    "    for idx in ids:\n",
    "        current_predictions_history = np.array(H[idx])\n",
    "        last_row_index = len(current_predictions_history) - 1\n",
    "        for j in range(label_dim):\n",
    "# 计算差分（除了第一个元素，因为差分是相邻元素的差）  \n",
    "            diffs = np.abs(np.diff(current_predictions_history[:, j]))\n",
    "            mean_diffs = np.sum(diffs)/len(diffs)\n",
    "            current_entropy = -1 / np.log(2) * (current_predictions_history[last_row_index][j]  * np.log(current_predictions_history[last_row_index][j]) + (1 - current_predictions_history[last_row_index][j] ) * np.log(1 -current_predictions_history[last_row_index][j] ))\n",
    "            E[idx,j]=1/2*mean_diffs+1/2* current_entropy\n",
    "    return E\n",
    "\n",
    "def update_U(E, U, epoch):\n",
    "    E[E > 1] = 1\n",
    "    if epoch>=5:\n",
    "        bins = np.array([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        discrete_values = np.array([0.1, 0.3, 0.5, 0.7, 0.9])\n",
    "        indices = np.digitize(E, bins) - 1\n",
    "        indices = np.clip(indices, 0, len(discrete_values) - 1)\n",
    "        E = discrete_values[indices]\n",
    "#         I=np.abs(calculate_pearson_matrix(E))\n",
    "        \n",
    "    else:\n",
    "        I = np.ones((E.shape[1],E.shape[1]))\n",
    "#     if epoch==30:\n",
    "#         np.save('yahoo-Business1I(30).npy', I)\n",
    "    I = np.ones((E.shape[1],E.shape[1]))\n",
    "    U = np.dot(E, I)\n",
    "    w = np.sum(U, axis=1)\n",
    "    return w\n",
    "\n",
    "\n",
    "\n",
    "def calculate_probabilities(U, epoch, e_end, e0, se0):\n",
    "    # Delta is set to 1/N\n",
    "    N=len(U)\n",
    "    Delta = 1 / N \n",
    "    # Exponentially decay the selection pressure as training progresses\n",
    "    se = se0 * (np.exp(np.log(1 / se0) / (e_end - e0)))**(epoch - e0)\n",
    "    U_min = np.min(U)\n",
    "    U_max = np.max(U)\n",
    "    U_normalized = (U - U_min) / (U_max - U_min)\n",
    "    quantized_indices = np.ceil((1 - U_normalized) / Delta).astype(int)\n",
    "#     pdb.set_trace()\n",
    "    # Calculate the exponent for the sampling probability\n",
    "    exponent = -np.log(se) / N * quantized_indices \n",
    "    # Calculate the unnormalized probabilities\n",
    "    unnormalized_probabilities = np.exp(exponent)\n",
    "    return unnormalized_probabilities\n",
    "\n",
    "\n",
    "def calculate_mutual_information_matrix(data):\n",
    "    n, q = data.shape\n",
    "    mi_matrix = np.zeros((q, q))\n",
    "    for i in range(q):\n",
    "        for j in range(q):\n",
    "            if i == j:\n",
    "                mi_matrix[i, j] = 1  # 对角线置为1\n",
    "            else:\n",
    "                mi_matrix[i, j] = mutual_info_classif(data[:, i].reshape(-1, 1), data[:, j])[0]\n",
    "                mi_matrix[i, j] = max(mi, 0)\n",
    "    return mi_matrix\n",
    "\n",
    "def calculate_pearson_matrix(data):\n",
    "    pearson_matrix = np.corrcoef(data, rowvar=False)\n",
    "    return pearson_matrix\n",
    "\n",
    "def entropy_based_mutual_information_matrix(data):\n",
    "    n, q = data.shape\n",
    "    mi_matrix = np.zeros((q, q))\n",
    "    \n",
    "    for i in range(q):\n",
    "        for j in range(i, q):\n",
    "            if i == j:\n",
    "                mi_matrix[i, j] = 1\n",
    "            else:\n",
    "                mi = entropy_based_mutual_information(data[:, i], data[:, j])\n",
    "                mi_matrix[i, j] = mi\n",
    "                mi_matrix[j, i] = mi\n",
    "    \n",
    "    # Normalize each row by its maximum value\n",
    "    row_max = mi_matrix.max(axis=1, keepdims=True)\n",
    "    mi_matrix = mi_matrix / row_max\n",
    "    \n",
    "    return mi_matrix\n",
    "\n",
    "def entropy_based_mutual_information(x, y):\n",
    "    # Calculate mutual information between x and y using entropy-based methods\n",
    "    # Ensure x and y are flattened to 1D arrays\n",
    "    x = np.ravel(x)\n",
    "    y = np.ravel(y)\n",
    "    \n",
    "    # Calculate individual entropies\n",
    "    h_x = entropy(np.histogram(x, bins='auto')[0])\n",
    "    h_y = entropy(np.histogram(y, bins='auto')[0])\n",
    "    \n",
    "    # Calculate joint entropy\n",
    "    joint_hist, _, _ = np.histogram2d(x, y, bins='auto')\n",
    "    joint_entropy = entropy(joint_hist.ravel())\n",
    "    \n",
    "    # Mutual information\n",
    "    mi = h_x + h_y - joint_entropy\n",
    "    return mi\n",
    "\n",
    "class CustomDataLoader:\n",
    "    def __init__(self, dataset, net, configs,H,E,U,sample_probabilities,warm_epoch):\n",
    "        self.shuffle = False\n",
    "        self.dataset = dataset\n",
    "        self.net = net\n",
    "        self.batch_size = configs['batch_size']\n",
    "        self.sample_probabilities =sample_probabilities\n",
    "        self.dataset_indices=list(range(len(self.dataset)))\n",
    "        self.warm_epoch=warm_epoch   \n",
    "        \n",
    "        self.H=H\n",
    "        self.E=E\n",
    "        self.U=U    \n",
    "                              \n",
    "    def __iter__(self):\n",
    "        if self.current_epoch < self.warm_epoch:\n",
    "            batch_counter = 0\n",
    "            for start_idx in range(0, len(self.dataset_indices), self.batch_size):\n",
    "                end_idx = min(start_idx + self.batch_size, len(self.dataset_indices))\n",
    "                batch_indices = self.dataset_indices[start_idx:end_idx]\n",
    "                batch_counter += 1\n",
    "                yield self.get_data_from_indices(batch_indices)\n",
    "        else:\n",
    "            all_indices = self.dataset_indices  \n",
    "            all_probabilities = self.sample_probabilities   \n",
    "\n",
    "            total_sum = all_probabilities.sum()\n",
    "            all_probabilities /= total_sum\n",
    "\n",
    "            total_samples = len(self.dataset)\n",
    "            num_batches = np.ceil(total_samples / self.batch_size).astype(int)\n",
    "\n",
    "            for _ in range(num_batches):\n",
    "                num_required = self.batch_size \n",
    "                if num_required > 0:\n",
    "                    chosen_indices = np.random.choice(all_indices, size=num_required, replace=False, p=all_probabilities)\n",
    "                else:\n",
    "                    chosen_indices = []\n",
    "                batch_indices = list(chosen_indices)\n",
    "                yield self.get_data_from_indices(batch_indices)\n",
    "                \n",
    "    def get_data_from_indices(self, indices):\n",
    "        x, y = zip(*[self.dataset[i] for i in indices])\n",
    "        return indices, torch.stack(x), torch.stack(y)\n",
    "    def set_epoch(self, epoch):\n",
    "        self.current_epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "870e4e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T13:53:32.744188Z",
     "start_time": "2024-07-31T13:18:29.373708Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bibtex\n"
     ]
    }
   ],
   "source": [
    "def training(configs,test_epoch):\n",
    "    net=CLIFModel(configs).to(device)\n",
    "    num_epochs =configs['epoch']\n",
    "    batch_size = configs['batch_size']\n",
    "    card=int(configs['card'])\n",
    "#     card=configs['num_classes']\n",
    "    lr = 1e-4\n",
    "    \n",
    "    ins_dim=configs['num_ins']\n",
    "    label_dim=configs['num_classes']\n",
    "    \n",
    "    weight_decay = 1e-4  \n",
    "    betas = (0.9, 0.999) \n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr,  \n",
    "                           betas=betas, weight_decay=weight_decay)\n",
    "    writer = SummaryWriter(comment=f'{dataname}')\n",
    "\n",
    "    sample_probabilities=[]\n",
    "    sample_indices=list(range(ins_dim))\n",
    "    H = {idx: deque(maxlen=5) for idx in sample_indices}\n",
    "    E= np.zeros((ins_dim, label_dim))\n",
    "    U = np.zeros(ins_dim)\n",
    "    U_all= np.zeros(ins_dim)\n",
    "    warmup_epochs =5\n",
    "    custom_dataloader = CustomDataLoader(train_dataset, net=net, configs=configs,H=H,E=E,U=U,sample_probabilities=sample_probabilities,warm_epoch=5)\n",
    "    validation_dataset, test_dataset_new = train_test_split(test_dataset, test_size=0.5, random_state=42)   \n",
    "    test_dataloader = DataLoader(test_dataset_new, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    best_auc = 0\n",
    "    best_model_state = None\n",
    "    epoch_losses_train = []\n",
    "    all_loss_list=[]\n",
    "    \n",
    "    \n",
    "    total_steps = num_epochs * int(configs['num_ins']/batch_size)\n",
    "    warmup_steps = warmup_epochs * int(configs['num_ins']/batch_size)\n",
    "    global_step = 0\n",
    "    def update_learning_rate(optimizer, global_step, warmup_steps=warmup_steps, base_lr=lr):\n",
    "        if global_step < warmup_steps:\n",
    "            lr = base_lr * (global_step / warmup_steps)\n",
    "        else:\n",
    "            lr = base_lr\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr \n",
    "    all_loss_list=[]   \n",
    "    mean_U=[]\n",
    "    for epoch in range(num_epochs+1): \n",
    "        net.train()\n",
    "        batch_counter = 0   \n",
    "        loss_tracker = 0.0\n",
    "        loss_tracker2=0.0\n",
    "        \n",
    "        custom_dataloader.set_epoch(epoch) \n",
    "        for idx, x, y in custom_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(x)\n",
    "            loss_dict = net.loss_function_train(outputs, y)\n",
    "            loss = loss_dict['Loss']\n",
    "#             分析所有样本的loss分布          \n",
    "            if epoch == test_epoch:\n",
    "                individual_losses1 = net._loss_per_label(outputs[configs['out_index']], y)\n",
    "                for i in range(individual_losses1.shape[0]):\n",
    "                    all_loss_list.append(round(individual_losses1[i].item(), 3))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            update_learning_rate(optimizer, global_step) \n",
    "            custom_dataloader.H=update_H(custom_dataloader.H, torch.sigmoid(outputs[0]), idx)\n",
    "            custom_dataloader.E=update_E(custom_dataloader.H, custom_dataloader.E,idx,label_dim)\n",
    "            batch_counter+=1 \n",
    "        custom_dataloader.U=update_U(custom_dataloader.E, custom_dataloader.U, epoch)\n",
    "        custom_dataloader.sample_probabilities=calculate_probabilities(custom_dataloader.U, epoch, 100, 0, 10) \n",
    "        mean_U.append(np.sum(custom_dataloader.U)/len(custom_dataloader.U))\n",
    "        if epoch==99:\n",
    "            np.save('bibtex_mean_U.npy', mean_U)\n",
    "#             np.save('yahoo-Business1E(30)_1.npy', custom_dataloader.E)\n",
    "#             np.save('yahoo-Business1U(30)_1.npy', custom_dataloader.U)  \n",
    "        writer.add_scalar('train/loss', loss_tracker, epoch)\n",
    "        epoch_losses_train.append(loss_tracker /batch_counter)\n",
    "        net.eval()\n",
    "        y_true_list = []\n",
    "        y_scores_list = []\n",
    "        loss_tracker = 0.0\n",
    "        batch_counter = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in validation_dataloader:\n",
    "                _, y_pred=net.predict(x)\n",
    "                y_true_list.append(y.cpu().numpy())\n",
    "                y_scores_list.append(y_pred.cpu().numpy())\n",
    "                \n",
    "                outputs = net(x)\n",
    "                loss_dict = net.loss_function_train(outputs, y)\n",
    "                loss = loss_dict['Loss']\n",
    "                loss_tracker += loss.item()\n",
    "                batch_counter += 1\n",
    "        y_true = np.vstack(y_true_list)\n",
    "        y_scores = np.vstack(y_scores_list)\n",
    "\n",
    "        auc = macro_averaging_auc(y_true,y_scores, y_scores)\n",
    "#         writer.add_scalar('val/auc', auc, epoch)\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_epoch=epoch\n",
    "            best_model_state = net.state_dict().copy()\n",
    "    net.load_state_dict(best_model_state)\n",
    "    mets = eval_metrics(net, [macro_f1, micro_f1, macro_averaging_auc, ranking_loss, hamming_loss, one_error], test_dataset_new, configs['out_index'],configs['batch_size'],torch.device('cuda'))\n",
    "    return all_loss_list\n",
    "\n",
    "\n",
    "# path_to_arff_files = [\"emotions\",\"scene\",\"yeast\", \"Corel5k\",\"rcv1subset1\",\"rcv1subset2\",\"rcv1subset3\",\"yahoo-Business1\",\"yahoo-Arts1\",\"bibtex\",'tmc2007','enron','cal500','LLOG-F']\n",
    "# label_counts = [6, 6,14,374,101,101,101,28,25,159,22,53,174,75]\n",
    "# select_feature=[1,1,1,1,0.02,0.02,0.02,0.05,0.05,1,0.01,1,1,1]\n",
    "\n",
    "path_to_arff_files = [\"bibtex\"]\n",
    "label_counts = [159]\n",
    "select_feature=[1]\n",
    "sample_loss_dict={}\n",
    "\n",
    "\n",
    "for idx, dataname in enumerate(path_to_arff_files):\n",
    "    path_to_arff_file = f\"/home/tt/{dataname}.arff\"\n",
    "    X, y, feature_names, label_names = load_from_arff(\n",
    "        path_to_arff_file,\n",
    "        label_count=label_counts[idx],\n",
    "        label_location=\"end\",\n",
    "        load_sparse=False,\n",
    "        return_attribute_definitions=True\n",
    "        )\n",
    "    X,feature_names=FeatureSelect(X,select_feature[idx])  \n",
    "    y,label_names=LabelSelect(y)\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    warm_epoch=10\n",
    "    print(dataname)\n",
    "    k_fold = IterativeStratification(n_splits=5,order=1,random_state=42)\n",
    "    dicts=[]\n",
    "    for idxx,(train,test) in enumerate(k_fold.split(X,y)):\n",
    "        if idxx==4:\n",
    "            train_dataset = TensorDataset(torch.tensor(X[train], device=device, dtype=torch.float),torch.tensor(y[train], device=device,dtype=torch.float))\n",
    "            test_dataset = TensorDataset(torch.tensor(X[test], device=device, dtype=torch.float), torch.tensor(y[test], device=device, dtype=torch.float))    \n",
    "            configs=CFG('CLIF',X[train],y[train]).getconfig()\n",
    "            result=training(configs,30)\n",
    "            sample_loss_dict[dataname]=result\n",
    "#             epoch_loss_dict_train[dataname]=epoch_losses_train\n",
    "#             epoch_loss_dict_val[dataname]=epoch_losses_val\n",
    "#             epoch_auc_dict_val[dataname]=epoch_auc_val\n",
    "#             print(best_epoch)\n",
    "# with open('epoch_loss_dict_train0.pickle', 'wb') as f:\n",
    "#     pickle.dump(epoch_loss_dict_train, f)\n",
    "# with open('epoch_loss_dict_val0.pickle', 'wb') as f:\n",
    "#     pickle.dump(epoch_loss_dict_val, f)\n",
    "# with open('epoch_auc_dict_val0.pickle', 'wb') as f:\n",
    "#     pickle.dump(epoch_auc_dict_val, f)\n",
    "# with open('batch_loss_dict_train1.pickle', 'wb') as f:\n",
    "#     pickle.dump(batch_loss_dict_train, f)\n",
    "# with open('Figure/sample_loss_dict_Uncertain(30).pickle', 'wb') as f:\n",
    "#     pickle.dump(sample_loss_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea85613",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T07:21:41.417961Z",
     "start_time": "2024-07-02T07:21:41.417931Z"
    }
   },
   "outputs": [],
   "source": [
    "path_to_arff_files = [\"bibtex\"]\n",
    "label_counts = [159]\n",
    "select_feature=[1]\n",
    "sample_loss_dict={}\n",
    "\n",
    "\n",
    "for idx, dataname in enumerate(path_to_arff_files):\n",
    "    path_to_arff_file = f\"/home/tt/{dataname}.arff\"\n",
    "    X, y, feature_names, label_names = load_from_arff(\n",
    "        path_to_arff_file,\n",
    "        label_count=label_counts[idx],\n",
    "        label_location=\"end\",\n",
    "        load_sparse=False,\n",
    "        return_attribute_definitions=True\n",
    "        )\n",
    "    X,feature_names=FeatureSelect(X,select_feature[idx])  \n",
    "    y,label_names=LabelSelect(y)\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    warm_epoch=10\n",
    "    print(dataname)\n",
    "    k_fold = IterativeStratification(n_splits=5,order=1,random_state=42)\n",
    "    dicts=[]\n",
    "    for idx,(train,test) in enumerate(k_fold.split(X,y)):\n",
    "        if idx==4:\n",
    "            train_dataset = TensorDataset(torch.tensor(X[train], device=device, dtype=torch.float),torch.tensor(y[train], device=device,dtype=torch.float))\n",
    "            test_dataset = TensorDataset(torch.tensor(X[test], device=device, dtype=torch.float), torch.tensor(y[test], device=device, dtype=torch.float))    \n",
    "            configs=CFG('CLIF',X[train],y[train]).getconfig()\n",
    "            result=training(configs,30)\n",
    "            sample_loss_dict[dataname]=result\n",
    "#             epoch_loss_dict_train[dataname]=epoch_losses_train\n",
    "#             epoch_loss_dict_val[dataname]=epoch_losses_val\n",
    "#             epoch_auc_dict_val[dataname]=epoch_auc_val\n",
    "#             print(best_epoch)\n",
    "# with open('epoch_loss_dict_train0.pickle', 'wb') as f:\n",
    "#     pickle.dump(epoch_loss_dict_train, f)\n",
    "# with open('epoch_loss_dict_val0.pickle', 'wb') as f:\n",
    "#     pickle.dump(epoch_loss_dict_val, f)\n",
    "# with open('epoch_auc_dict_val0.pickle', 'wb') as f:\n",
    "#     pickle.dump(epoch_auc_dict_val, f)\n",
    "# with open('batch_loss_dict_train1.pickle', 'wb') as f:\n",
    "#     pickle.dump(batch_loss_dict_train, f)\n",
    "# with open('Figure/sample_loss_dict_Uncertain(70).pickle', 'wb') as f:\n",
    "#     pickle.dump(sample_loss_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e005c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
